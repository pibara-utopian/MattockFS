\chapter{Evidence inter-disk-io timing in OCFA}
This appendix describes the fact-finding case-study regarding inter-job timing in the Open Computer Forensics Architecture (OCFA). While use and development of OCFA has been discontinued by the Dutch National Police, in the period from 2004 until 2012, this organization made has made extensive use of this asynchronous computer forensic framework. While no longer active, some old database backups were still available at the time of this study, and timing information has been extracted from these database dumps in order to study the real life disk access properties of such an asynchronous architecture. This chapter gives some background information on the OCFA messaging and routing mechanisms and on those facilities that OCFA implemented in order to prioritize processing of certain data in an attempt to optimize throughput. It than goes on to evaluate the timing information as extracted from the database dumps from five distinct investigations, and will try to reason about some possible explanations regarding the results of the analysis. This chapter will be ending with conclusions about the effectiveness of the OCFA throughput measures and areas of concern regarding disk-cache misses as far as these are relevant to the subject of this dissertation.
\section{Key points about the OCFA Architecture}
\subsection{Reasons for the asynchronous design}
In computer forensic frameworks concurrency and parallelism are important aspects. Many computer forensic tasks, like Optical Character Recognition (OCR) of scanned documents are CPU intensive, while others like creating a full-text search-able index are IO-intensive and some like the calculation of hashes on data are both. There is a tension between optimizing a forensic framework for IO efficiency and optimizing it for the effective use of available CPU power. There also is a tension between optimizing a forensic framework for single-node performance and making the framework scale to make use of a cluster of workers for CPU intensive tasks. When we look at the base idea of concurrency, there are two models by witch a concurrent system can operate: 
\begin{itemize}
\item Shared state concurrency.
\item Message passing concurrency.
\end{itemize}
Shared state concurrency comes with many advantages in the light of the tension between IO efficiency issues, yet message passing concurrency is fundamental better suited for the multi-server high-CPU module scenario. For OCFA it was a different non-functional requirement that tipped the scale towards the use of message passing security: \emph{robustness in the presence of local failure}. OCFA was meant to be used in
a highly dynamic and reactive environment where investigations could not wait for a development team to create a robust well tested module at the moment the need arose. No, in OCFA, the use of highly targeted \emph{quick and dirty} module development was an essential feature. A feature however that meant that a module could and would occasional just crash. Both locality of failure and crash recovery ended up being primary specifications for the OCFA architecture and the choice for message passing concurrency flowed naturally from that. A choice that as this appendix will show cam at a price.
\subsection{High-level routing of jobs}
Messaging in the OCFA framework took place at two distinct levels of abstraction. At the highest level of abstraction, meta-data was passed between module \emph{functionalities} by a process (or set of processes) called the XML-Router. This router would evaluate the meta-data generated by previous modules and would based on that meta-data determine what module should come next. We shall see in our results and evaluation below that some aspects of the XML-router implementation possibly ended up hindering primary performance in an important way.
\subsection{Low-level relaying of jobs}
While the high-level XML-routing was essential to the OCFA framework as messaging mechanism, and while the routing system as we shall show later on did have an impact on performance, the main message-passing concurrency system for OCFA was constituted by the low level relaying system. This system consisted of two main components:
\begin{itemize}
\item The Persistent Priority Queue library.
\item The Anycast Relay
\end{itemize}
The Anycast relay was a single process implementation of what today would be called a message bus. It shared many characteristics with modern message bus systems like RabitMQ, yet specifically tailored for the OCFA systems robustness and crash-recovery needs. A full description of the crash-recovery features of the Anycast relay falls outside of the scope of this document, but it is sufficient to state that the Anycast relay would set-aside unprocessed and partially processed messages from crashed module instances to both allow for localization of failure and for solid crash recovery. The Anycast relay was a simple messaging server with a persistent per module-type message-queue. Within the 
Anycast relay messaging system each module would both be consumer (based on its module functionality) and producer in a multi-producer multi-consumer set-up. 
For the purpose of this document, the Anycast Relay should be seen as a set of channels between producers and consumers or consumer pools. As is a well known problem with buffer constrained systems, asynchronous message passing systems like OCFA can suffer from situations that truly stretch the capacity needs for message buffers. The Persistent Priority Queue library solves these issues for OCFA in a way that prioritizes robustness and delivery guarantees over performance concerns. We shall later evaluate the choices made for OCFA and how changes in these choices could potentially improve the way a computer forensic framework could attenuate the buffering issue that seem to be inherent to the choice for message passing concurrency. 
\subsection{Job priorities and queuing}
The main strategy applied by OCFA in order to improve the throughput of the overall architecture is the use of priorities. We shall later see that while according to some measurements that focus on the per-event timing, these measures would seem effective, yet when focussing on other measurements, namely those for per volume timing, these measures turn out to be highly inadequate. The base idea of the job priorities for OCFA was that each non-router module would increment the priority of the processed data. While this concept worked on a link by link basis, it did help to allow individual modules (including the router) to prioritize the oldest data, thus allowing the framework to reduce the amount of cumulative active data in the system. 
\subsection{Timestamps in the OCFA databases}
The XML Blobs created and extended by the OCFA modules don't contain just per-job meta-data. They are also used to store profiling information listing the start time and stop time of every module with a one second granularity.  These XML blobs are all stored in the Postgress database that is used as central component in the OCFA infrastructure. From this profiling information it should be possible to extract some basic statistics that should proof useful in analyzing and evaluating the disk-cache-miss likelihood an possibly come up with the most suitable remedies for improving upon these.
\section{Analysis of the OCFA timing information}
\subsection{Extracting the timing information}
The Dutch National Police made a set of database dumps from old OCFA runs available for use in this study. One important prerequisite however was that the sensitive investigative data could under no circumstances become part of the scientific research. In order to become able to adhere to these preconditions, a Python script was created to parse the database dumps and to extract only those parts of the data that were essential for \emph{technical} evaluation related to timing. In theory this information should be sufficient to base our research and evaluations on, yet it is possible that a better foundation of our findings would be possible when including non-directly timing related information. The python script as included in a following appendix basically did the following:
\begin{itemize}
\item Extract the evidence XML Blob's from the SQL dump.
\item Extract the oldest timestamp from the XML blobs and use that time as offset for the successive steps.
\item Process each individual XML blob and extract only the relative start and stop times and module types for each job.
\item Write the extracted information to a JSON based log file.
\end{itemize}
The true analysis of the OCFA timing information took place on the resulting JSON based log files. It is important to note that due to a bug in the OCFA Java library at the time these investigations were active, there are were some (detectable/reversible) offset bugs in the results. These needed to be worked around during the actual analysis.




