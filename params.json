{"name":"Mattockfs","tagline":"MattockFS Computer Forensics File-System","body":"### What is MattockFS?\r\n\r\nMattockFS is a Computer Forensics File-System. More accurately, it's a user-space file-system that aims to provide a major part of the underpinning of an envisioned message-passing based Computer Forensics Framework for use in a Computer Forensics laboratory. MattockFS aims to be a component in a Computer Forensics Framework aimed at medium to large scale investigations that is suitable for both professional investigation settings yet also suitable as basis for a framework that takes into account the need for compatibility with a more academic setting. MattockFS is not a full Digital Forensics Framework. Rather, MattockFS is a central component for a three part Computer Forensics Framework. It performs multiple key functionalities that a Computer Forensics Framework requires, and fulfills a linking function between the other two sub-frameworks.\r\n\r\n### MattockFS in context. \r\nBefore we describe MattockFS itself, we shall briefly visit the base components of the envisioned Computer Forensics Framework that MattockFS aims to be part of. We envision a distributed open-source Computer Forensics Framework that we shall refer to as The Mattock Computer Forensics Framework, or just Mattock for short. This framework is loosely modeled after the Open Computer Forensics Architecture (OCFA), an old Computer Forensic Framework build and open-sourced by the Dutch National Police that as a project has been orphaned many years ago. One of the core qualities of OCFA was the fact that it allowed for the integration of forensic and non-forensic data processing modules into dynamic data-processing tool-chains. Mattock aims to do the same, but using a more modern system design and applying current day data processing technology and computer forensic insights as well as over a decade worth of lessons-learned from the use of OCFA in investigations. We shall not go into full detail with respect to Mattock. All we will discus here is the top-level design of Mattock. More specifically, the way Mattock is defined to be the combination of three main sub-frameworks. The first thing we need to realize is that Mattock (like OCFA before it) aims to be a Computer Forensics Framework that allows a tool-chain for any particular piece of data to be dynamically determined based on content and meta-data. the second thing to realize is that Mattock (like OCFA before it) takes the process as module-instance approach. A module instance is a single operating system process that acts as a worker for processing a particular type of data in a particular way. An installation may run a single instance of a particular module, or it may run multiple instances, either on the same system or on a cluster of Mattock server node's. Unlike OCFA, Mattock takes data locality very seriously. That is, data prefers to stick to being processed on a single node and tool-chains containing a node switch are aimed to be scares. So now that we have a base idea of how Mattock aims to work for the Computer Forensics Process, we shall look at our three sub-frameworks:\r\n\r\n* **MattockFS**: Data repository,access control, local message-bus and provenance logging.\r\n* **MattockLib**: Distributed router, meta serialization, evidence tree walking, file-type checking and throttling. \r\n* **MattockNet**: Networked/balanced data-input. Inter Mattock-node load-balancing.\r\n\r\nIt is important to note that at time of writing, MattockFS was in early aplha and neither MattockLib nor MattockNet had even one line of code written for it. I hope to make Mattock as a whole a community effort, so if you feel inclined to start a new open-source computer forensics project, MattockLib or MattockNet might be something to consider.\r\n\r\n**MattockFS** will provide the per-node access to one of the data repositories, more on this below. While read-access is provided globally, MattockFS provides strict context and access control for adding and unforgeable provenance logged access to any writing operations. Combined with these data-input controls, MattockFS implements the message-bus functionality that in OCFA was handled by the Anycast Relay. It does so in the form of an in-file-system local system message bus for handing over data chunks to the next module in the tool-chain. MattockFS also provides both MattockLib and MattockNet with usefull page-cache and anycast load statistics that these systems may use for throttling and/or load-balancing purposes. \r\n\r\n**MattockLib** is the part of the framework that allows modules to communicate with one or multiple MattockFS instances. It allows modules to register with MattockFS as an instance of a particular module. To accept a job from the MattockFS per-module any-cast bus, to store derived (meta-)data in a suitable repository, and to determine the next hop in the tool-chain based on distributed router functionality within MattockLib that allows MattockLib to determine the next hop based on tool-chain router state and newly acquired meta-data from the module. Mattocklib is also responsible properly serializing module meta-data for applying libmagic logic to any indeterminate content type. MattockLib is also responsible for working with the page-cache, anycast and CPU load stats from the underlying system in order to adjust its interaction with MattockFS and for throttling new-data input if appropriate. The most important thing that MattockLib provides is a standardized API for data processing modules. It is strongly suggested to anyone wanting to implement a MattockLib implementation to use the old OCFA tree-graph API as basis for an asynchronous lambda based API design.\r\n\r\n**MattockNet** provides a mesh-up network between the different MattockFS instances on the network. It provides a system-load based system for networked input of digital forensic input data, and provides monitoring of system load for all the servers in the mesh-up. If there is a big difference in system load and if there are specifically suitable jobs in the anycast buses of the overloaded node, than MattockNet shall take action to migrate specific jobs to a different node.\r\n\r\n### The underlying archive.\r\nCurrently the underlying archive for MattockFS is a single large sparse file under /var/mattock/archive/0.dd . This is suitable for single-node experimental setups, but for future multi-node setups that work with MattockNet, at least two alternatives need to be implemented;\r\n\r\n* **Centralized fiber-channel storage** : As in most OCFA setups at the Dutch Police in the past, the use of centralized storage through fiber-channel and StoreNext file-system could provide for a simple distributed MattockFS setup where all nodes run a single instance of MattockFS on top of the same SNFS hosted raw sparse archive.\r\n* **NFS mesh-up** : An alternative low-budget solution that could be used instead of StoreNext, and that should be much more suitable for Mattock than it was for OCFA is the use of an NFS mash-up. It must be noted that while an NFS mesh-up works pretty well, it does take considerable time to set it up. In an NFS mesh-up, every node runs one master MattockFS instance and multiple slave MattockFS instances. The master runs on a locally accessed archive that is also made available (read-only) to slaves running on other node's. \r\n\r\nThe final version of MattockFS should support both these mode's. It is suggested also that other options such as erasure-encoded distributed storage should be explored. The use of alternative (forensic) storage formats should also be considered an important topic of future study.\r\n\r\n### CarvPath annotations\r\nIf there is one joining property of all of the MattockFS features, it must be the use of CarvPath annotations. MattockFS's predecessor CarvFS introduced the concept of CarvPath annotations. A CarvPath annotation is a simple string token, suitable for use as pseudo-file-name, that encodes a list of one or more sparse or real fragments within a larger entity. In MattockFS, under the $mp/data directory, Carvpath annotations are used to designate pseudo-files within the archive. Before we discuss how these are designated in MattockFS, we shall discuss the basics of CarvPath annotations.\r\n\r\n* A single fragment is designated with the form $OFFSET+$SIZE, for example '0+512' designates the first 512 bytes of the underlying entity.\r\n* A single chunk of sparse data is designated with the form S$SIZE, for example 'S4096' designates a sparse chunk of 4096 bytes in length.\r\n* A multi-chunk entity is designated by joining together chunk designations with an underscore character. For example: '0+4096_S8192_4096+4096' designates two chunks of data separated with a chunk of sparse. \r\n* A multi-chunk entity with a string length exceeding what is deemed to be a reasonable length is designated with the form D$DIGEST, where digest is the BLAKE2 hash of the designation that was deemed too long for usage.\r\n* If an entity is relative to an other CarvPath designated entity, than joining the relative child entity designation with the parent designation using a slash character results in a valid CarvPath annotation. For example '0+4096_S8192_4096+4096/2048+4096' designates a valid sub-section of our earlier CarvPath.\r\n\r\nSo how are these CarvPath designations used in MattockFS? The directory structure is quite straight forward and allows for nested CarvPath usage. The directory $MP/data is an un-listable directory. Two valid entities can be designated within $MP/data. A valid single level CarvPath with a file extension (any extension will do), designates a usable pseudo file. Without a file-extension, a sub-directory is designated that also is unlistable. Within this sub-directory, any valid sub-entity designating CarvPath, with or without file extension, will designate a symbolic link that points to a flattened version of the designated two level CarvPath. For example '0+4096_S8192_4096+4096/2048+4096.dat' would become a symbolic link to '../2048+2048_S8192_4096+2048.dat'. This setup allows for arbitrary long nesting constructs as each nesting level is immediately flattened by the symbolic link.\r\n### Long paths\r\nThe use of long paths and their shortened representation poses a problem that may not be directly obvious. As the long path itself can often not be communicated between the Mattock module and the file-system, and as it can not be derived back from just the hash, we need a place of shared storage that is shared between the module instance and all relevant instances of the MattockFS file-system. The current implementation of MattockFS uses Redis as key/value store and expects the Mattock module to do the same. \r\n### The tool-chain and overlapping hot CarvPath annotations.\r\nThe second most important concept within MattockFS is the concept of the tool-chain. We consider the processing of a single CarvPath annotated chunk of archive to follow the path of a dynamic tool-chain. As long as the tool-chain for any given CarvPath has not been fully completed, we consider the CarvPath to be _hot_. After the last module in the toolchain completes, the CarvPath becomes _cold_, and MattockFS takes appropriate measures (calling possix_fadvise) to notify the kernel of this fact. Given that CarvPath annotations will often overlap (Think e-mail within mbox within ISO image within file-system partition within disk-image), a CarvPath may not actually become cold (or at least not completely become cold) when the last module is done with it. A parent or child CarvPath may still be hot. To work with this, MattockFS maintains a reference counting stack of hot toolchain CarvPath annotations that is tightly bound to the Anycast concept within MattockFS. If the reference count for a chunk of archive goes down to zero, it is marked as _cold_, if it goes from zero to one it is marked as _hot_. This process is logged for profiling purposes into /var/mattock/log/0.refcount .\r\n### The job\r\nin MattockFS, a tool-chain CarvPath will exist in an incarnation as a Job. A Job may be active (currently being processed by a module), or may be part of one of the Anycast job-sets. During these incarnations, the tool-chain will build up a provenance log that will become available once the tool-chain has completed.\r\n### file-system as API and privilege separation tool.\r\nIn the later days of OCFA, CarvFS was used for data-storage. The file-system however was fully read-only and ran under the exact same UID as the modules did and writing to the raw archive was done by the kickstarting module rather than the file-system. In MattockFS we take a different approach. MattockFS runs under its own private UID and tha raw archive is owned by this UID. Writing to the archive is done through the file-system under strict and independently logged conditions. We consider the use of a seperate UID and the use of the file-system as writing and logging deputy to be essential privilege separations constructs for the overall robustness of the Mattock framework. We take the conservative approach that a single unaudited third-party module having the power to corrupt the archive or the provenance log as a result of hostile-data processing should be considered an unacceptable risk for large-scale multi-module processing setups. The data corruption capacity of the archive data by a vulnerable tool should be limited to those parts of the archive ascribable to the vulnerable tool. We thus consider privilege separation and other access control measures that help to guard this desired framework property to be of the utmost importance. \r\n### Sealed/frozen mutable data\r\nThe first data-integrity measure that MattockFS implements is the concept of sealed or frozen data. MattockFS allows each module to allocate mutable archive storage, and only within the context of the tool-chain it is working on. The mutable data gets represented as a fixed-size writable file within MattockFS. A note to implementors: The file may be writable, it's not truncatable. After the module is finished creating the file, MattockFS allows the module to _freeze_ the mutable data and obtain a CarvPath to the frozen read-only version of the file. Just as a sub-CarvPath, this CarvPath can than be entered into MattockFS as first job in a new child-entity tool-chain, remaining immutable for the remaining modules in the chain.\r\n### Provenance log\r\nWhile MattockFS in general is meta-data achnostic (MattockLib generated meta-data is treated as just an other mime-type), our privileged separation needs require lab-side provenance related meta-data to be handled by MattockFS. We do this by creating a first provenance record when the first job in a tool-chain is created, appending a short provenance record for each consecutive job, and writing all records (as single-line JSON) to a provenance log file. Currently the provenance log is written to /var/mattock/log/0.provenance . In a final version of MattockFS it may proof useful to also route provenance-meta entities to a data-store module. There currently are some design considerations hindering an implementation of this feature though.\r\n### The Anycast message-bus\r\nIn OCFA the Anycast relay was based on a persistent priority queue implementation. In MattockFS the current Anycast implementation lacks both persistence and a working journaling system capable of restoring known good state. Implementation of a journal based restore should be considered an essential missing feature. In MattockFS we have abandoned the idea of using a priority queue. when analyzing old OCFA profiling logs, the priority queuing turned out not to yield the results desired with respect to page-cache hit-rates. Instead of infinitely grow-able queues, MattockFS provides MattockLib with hooks for throttling input instead. MattockLib can query MattockFS with respect to Anycast-set sizes and the amount of _hot_ job data in the system. On the other end, MattockFS allows MattockLib to ask for a next job according to one or more job-select policies. Further study for optimum select-pollicies is needed and MattockFS tries to implement a few potentially useful policies in order to facilitate this research:\r\n\r\n* K : Kickstarting pollicy; create a job out of thin air and accept it.\r\n* R : Prefer jobs that contain CarvPath chunks with the highest currently active refcount.\r\n* r : Prefer jobs that contain chunks with a refcount of one.\r\n* O : Prefer jobs that contain chunks with the lowest possible offset.\r\n* H : Prefer jobs that contain chunks with the lowest possible opportunistic-hashing adjusted offset.\r\n* D : Prefer jobs with the lowest possible density of chunks with the maximum refcount\r\n* d : Prefer jobs with the lowest possible density of chunks with a refcount higher than one.\r\n* W : Prefer jobs with the lowest weighted average refcount over all of its chunks.\r\n* S : Prefer jobs with the smallest total data size.\r\n \r\nNot all of these policies are currently implemented and the future may show many of these policies to be a poor choice. There are multiple interdependent issues with choosing a policy and only full-scale testing with a functional Mattock framework will reveal the most optimum policy choices. The ultimate goal is to choose the policies that enable effective load balancing between nodes, limit the average page-cache miss rates, maximize the risk of spurious throttling  and optimize the opportunity for beneficial opportunistic hashing. As some of these goals could end up mutually exclusive, real-data testing will become essential.\r\n### The use of sparse capabilities\r\nIn computer-programming, the use of object references is common good. A reference, other than a raw pointer, can not be made up by tricks like guessing and pointer-arithmetic. A reference as such is unguessable, at least in commonly used memory safe languages. In a file-system most file names are guessable names. Given our privilege separation needs, we find ourselves in a situation where we desire a safe reference between a Mattock Module and program state maintained in MattockFS. We don't want a vulnerable corruptible module to be in the position where it could grab hold of and corrupt state belonging to other module instances. In order to avoid that, we use the concept of sparse capabilities for our MattockLib/MattockFS interaction. As shown in [MinorFS](http://www.linuxjournal.com/magazine/minorfs), just using sparse capabilities is insufficient a guard against such issues. MattockFS implements a sparse capability based interface as FS based API, but the use of mandatory access control (like AppArmor) should be considered to fully secure the forensic process from integrity attacks through vulnerable modules.\r\n### Opportunistic hashing\r\nGiven the extensive use of partially overlapping _hot_ CarvPath entities within MattockFs, and given the fact that each high-level read/write translates to a low-level read/write, we have the opportunity to try and make hashing an opportunistic process. Whenever a chunk of low-level data is accessed, its access may be transposed to ghost-IO actions on each of the hot entities that overlap the accessed section. Under specific conditions, these ghost-IO actions may be used to further an opportunistic hashing process for each of those entities. If at any point in time the opportunistic hashing reaches the full size of the entity, than opportunistic-hashing will have been fully successful. Further, at any point in time, the opportunistic hashing index of a hot entity may be queried, so that only the last part of a file would need to be read if it becomes needed by a module or routing action. All completed opportunistic hashing actions are available during the life-time of the tool-chain. Additionally, successful hashing is logged to \r\n/var/mattock/log/0.ohash\r\n### The file-system based API\r\nSo now that we have all the theory out of the way, lets show how this all comes together as a file-system. We define our API largely in the form of sparse capabilities, special '.ctl' pseudo files and extended attributes. Think of ctl files as objects, sparse capabilities as object references and extended attributes as methods. If you grasp these three methods, understanding the FS-level API of MattockFS, and translating it to a programming-language specific API should become relatively trivial. In some rare cases the extended attribute interface does not fully map to a method paradigm.\r\nCommunicating through extended attributes is limited to getter and setter functionality. To functionality that can be mapped as void functions with parameters and non-void functions without. This means that, perforce, a non-void function with parameters needs to be expressed as two separate operations. Further when we want to communicate more than one fields, we opt for a simple semicolon separated composite string to do so.\r\nSo let's look at the directory structure of our file-system first:\r\n\r\n* data/\r\n * $CARVPATH.$EXT\r\n * $CARVPATH/\r\n    * $CARVPATH.$EXT\r\n    * $CARVPATH\r\n* mattockfs.ctl\r\n* module/\r\n * $MODULENAME.ctl\r\n* instance/\r\n * $INSTANCESPARSECAP.ctl\r\n* job\r\n * $JOBSPARSECAP.ctl\r\n* newdata\r\n * $MUTABLEDATACAP.dat\r\n\r\nWe already discussed the whole _data/_ directory structure in the section on CarvPath annotations, so lets focus on the FS-based API part.\r\n\r\n**data/$CARVPATH.$EXT** : This read-only file contains all the data under the designated CarvPath.\r\n* opportunistic_hash : This attribute on a hot entity will contain the opportunistic hash (if known) and the hashing offset if opportunistic hashing is incomplete.\r\n* fadvise_status: This composit attribute contains the part of the CarvPath that is hot and the part that is cold. This info may be useful when re-submitting parts of the archive, something that is likely to be a realistic scenario when MattockFS is being used during development and/or in an academic setting.\r\n\r\n**mattockfs.ctl** : This control file gives access to two pieces of global full-file-system data. It has the following extended attributes:\r\n* fadvise_status : Composite of the total amount of _hot_ and total amount of _cold_ data in the whole archive.\r\n* user.full_archive : A CarvPath representing the entire archive as a whole.\r\n\r\n**module/$MODULENAME.ctl** : These per-modulename control files serve as a double point of access. One for registering a process as module instance and a second one for pre-submit information that may be relevant for throttling purposes. \r\n* weight : This settable parameter is used by MattockNet when picking jobs to migrate. it indicates the general weight of the module data when migrating to an other node. It is suggested that higher weight data should be a first candidate for migration.\r\n* overflow : This settable parameter is meant to indicate to MattockNet what minimum amount of entities to leave in the given Anycast set as a minimum.\r\n* anycast_status : Composit of the total number of jobs in the Anycast set and the total size of all entities in the queue.\r\n* instance_count : Number of currently registered instances of this module.\r\n* reset : Setter (value mist be \"1\"). Force tear-down all instance state. We may deprecate this once the code is more stable. \r\n* register_instance : Reading this will return a new instance sparsecap (complete with responsibility for cleaning it up when finished).\r\n\r\n**instance/$INSTANCESPARSECAP.ctl** : This control is meant to be the primary control object for a registered module instance. We have the following attributes:\r\n* job_select_policy : This settable property allows us to set the job selection policy as described above.\r\n* module_select_policy : Specifically for the load balance functionality of MattockNet, this settable property allows the setting of a policy for choosing what module anycast to steal a job from for migration to an other node. The specification of the policies for this is still fluid at this time.\r\n* unregister: This setter (vallue must be \"1\") should be invoked just prior to the module instance process exiting. It unregisters the module instance.\r\n* accept_job : Try to fetch a job for processing according to the job selection policy. this method, if successfull will return a Job sparse capability.\r\n\r\n**job/$JOBSPARSECAP.ctl** : This temporarily valid control represents a Job that the module is currently in the process of processing.\r\n* job_carvpath : This attribute contains the CarvPath of the entity we need to process.\r\n* routing_info : this attribute is a composite and its gettable (current pre-processin router state)and settable (complete job and forward to the next module in the tool-chain). It is a composit of the (current or next) module and a short router state string that is to be used for implementing distributed routing by MattockLib.\r\n* allocate_mutable : As indicated before, some functionality that is logically one needs to be split out due to the extended attribute as API setup. This setter allocates mutable storage and binds it to the current job as current_mutable.\r\n* current_mutable: This getter returns the sparsecap of the current mutable. Please note that this mutable should be written and frozen before allocating an other mutable.\r\n* frozen_mutable: Calling this getter will revoke the current mutable capability and will return the CarvPath of the frozen entity.\r\n* submit_child: This setter takes a composit of carvpath;nextmodule;routerstate;mimetype;entension and will submit the entity as new tool-chain to the module mentioned in nextmodule. \r\n\r\n\r\n**newdata/$MUTABLEDATACAP** : This temporary file is a mutable fixed-size entity that is writable but can not be truncated. The file may be written to sparsely during its lifetime.\r\n\r\n### Helping out / Support / Contact\r\nAs stated, MattockFS aims to become part of the envisioned community-effort project Mattock. A community page was made on [Google+](https://plus.google.com/communities/102487198908055860744). If you wish to contribute, or if you desire any information or support related to Mattock or MattockFS, please consider joining this G+ community.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}