\chapter{Page-cache in the Linux operating-system}
In this appendix we examine the core properties of the Linux page-cache and the possibilities for interacting with the page-cache and related memory layout parameters in a way that could limit the disk-cache miss rate of a multi-process system running on this OS. First we will look at the main memory usage strategy used by Linux and at the way that page-cache competes for RAM with other RAM consuming Linux subsystems. We will look at information we can extract from the virtual /proc filesystem and at POSIX API's that provide a way to interact with the way the page-cache works. We then move on to project this knowledge to our fictitious computer-forensic framework and discuss what measures can be taken to effectively interact with the Linux system in order to limit page-cache misses on a Linux system that is hosting such a framework. We close this appendix by reasoning about possible strategies that could be used by a computer-forensic framework in an attempt to optimize disk-cache hit percentages while at the same time not introducing other sources of stagnating throughput to the resulting setup.
\section{Allocation and use of page-cache}
RAM in a computer is a shared resource. It is a scarce resource that is managed by the operating system. The OS kernel itself runs in RAM as do the user processes. Both kernel/process code, heap and stack take up pieces of RAM and on a busy system some RAM pages may end up getting swapped out of RAM to an on-disk swap facility. Not all RAM is kept available to kernel and processes. Part of RAM is allocated to hold a copy of on-disk data and to act as a cache for file data that the Linux kernel assumes is likely to be read again in the foreseeable future, allowing for more efficient use of the relatively slow reading from hard disk media. An other part is allocated to temporary hold data pages that should eventually end up on disk and aren't synced to disk yet to improve the overall system performance related to issues with frequent short writes. The Linux kernel needs to balance the desire to avoid wasting disk-IO on swapping out process memory with the desire to not waste disk-IO on data that could have been cached. It's as if there is a rubber band between the process oriented RAM usage and the file-IO oriented RAM usage. As such, memory available to page-cache is not static. It depends on the process behavior of the different processes on the system. Our main interest in RAM for the purpose of this research is an interest in the part of the RAM used for the read-part of the page cache. While there is a lot to say about the use and tuning of write oriented caches, this falls outside of the scope of this paper and thus shall not be discussed here. 
\section{/proc/meminfo}
In the /proc filesystem, the pseudo file /proc/meminfo can be used to access kernel information about the size and use of the system RAM. Some potentially interesting variables that can be retrieved from this pseudo file are:
\begin{itemize}
\item \emph{Cached} : In-memory cache for files read from disk (the page-cache).
\item \emph{MemFree} : The total of memory that is free to be used for anything.
\item \emph{Active} : Recently used memory that ideally isn't reclaimed.
\item \emph{MemTotal} : The total usable system RAM.
\item \emph{SwapTotal} : Total amount of swap space available.
\item \emph{SwapFree} : The currently unused portion of the swap space.
\end{itemize}
The information from this pseudo file can be used to initialize or later tune our efforts at throttling data input into our system. 
\section{/sys/block/\$DEV/stat}
Other than directly looking at the memory usage divisions, the actual disk activity should be a good indication of how well the page-cache is performing across a long running operation. The system pseudo filesystem defines a directory under \emph{/sys/block} for every block device. In any such directory resides a pseudo file named \emph{stat} that contains information that should be useful in this sense. The pseudo file gives access to I/O statistics of a given block device. Each line contains the current disk stat variables for a single device. 
\begin{enumerate}
\item Number of reads completed.
\item Number of reads merged.
\item Number of sectors read.
\item Number of milliseconds spent reading.
\item Number of writes completed.
\item Number of writes merged.
\item Number of sectors written.
\item Number of milliseconds spent writing.
\item Number of I/Os currently in progress.
\item Number of milliseconds spent doing I/O
\item Weighted number of milliseconds doing I/O
\end{enumerate} 
For our research we are interested in the first four numbers. The reading behavior. If we are to compare strategies for disk-cache efficiency, a test run with a lower number of reads, a higher number of reads merged and a smaller number of milliseconds spent reading would be the preferred result. As disk-cache misses would result in undesirable additional reads and increased amount of time spent reading, a good page-cache efficient solution should result in those numbers going down. 
\section{Starting off with a clean slate}
If we want to repeatedly run tests on the page-cache behavior of the system, it will be important that we can reinitialize our page-cache so that a first test does not mess with a second or third test. So how do we reinitialize the page cache so we can start our test with a clean slate? There are two things we need to do.  First make sure all page cache \emph{can} be initialized cleanly by invoking \emph{sync} as root. After that we run the following command (as root):
\begin{itemize}
\item \emph{echo 3 > /proc/sys/vm/drop\_caches}
\end{itemize}
This command will write the number \emph{3} to the pseudo file /proc/sys/vm/drop\_caches. Doing so will prompt the kernel to free the cache and slab objects. Finally we should save a copy of both \emph{/proc/meminfo} and the relevant \emph{/sys/block/\$DEV/stat} files before we can run our test. After running the test, an second copy can be made of these pseudo files. The differences between the two should yield information related to the page-cache mis/hit-rates.
\section{API interaction with the page-cache part of the Linux kernel}
\subsection{posix\_fadvice}
The Posix API call \emph{posix\_fadvise} or \emph{posix\_fadvice64} advises the kernel about the expected behavior of the application with respect to a region of data as associated with an open file descriptor. This advice helps the kernel in its page-cache behavior, as it will allow the kernel to determine if specific pages should get precedence over others when deciding what old page-cache data to keep and what to drop. A few important values for the advice are:
\begin{itemize}
\item \emph{POSIX\_FADV\_WILLNEED} : The data is expected to be accessed in the near future
\item \emph{POSIX\_FADV\_DONTNEED} : The data is NOT expected to be accessed in the near future
\item \emph{POSIX\_FADV\_NOREUSE} : The data is expected to only be accessed once.
\end{itemize}
\subsection{mincore}
It is possible for the application itself to find out if data it wants to access is currently present in the page cache. To do this, the application can invoke the \emph{mincore} API call.  
\subsection{readahead}
If the application knows that a file-descriptor is about to read a specific range of data, it may populate the page-cache explicitly by invoking the readahead Linux specific API call.
\section{Interaction between a computer-forensic framework and the Linux kernel}
When we assume a CarvFS-like user space file-system and OCFA-like routing and relaying architecture will be used, and if we look at the capabilities and knowledge that these and other subsystems may have, we get to the following list of facts:
\begin{itemize}
\item The CarvFS-like user space file-system, assuming that it keeps a file descriptor opened to the underlying data archive file, is the only place where API based interaction with the page-cache kernel subsystem is effective.
\item The CarvFS-like user space file-system can find out what type of module is accessing it when a pseudo file is opened for reading and could perform a last moment posix\_fadvise64 invocation that may be of some benefit. 
\item Knowledge about \emph{active} CarvPath's within the framework context, is not directly available to the CarvFS-like user space file-system and must come from explicit interaction of either the modules involved or the routing and messaging infrastructure.
\item With this knowledge, the CarvFS-like user space file-system can use invocation of the \emph{posix\_fadvice} API in a more structural way, properly marking the designated areas according to their projected remaining cross-module paths and usage. 
\end{itemize} 
\section{Possible strategies}
If we assume that a future computer-forensic framework will take the advice from the previous appendix at heart and will integrate the routing functionality in the libraries used by the different modules, we suggest that the ultimate strategy for page-cache interaction would be definable inside of the routing rule definitions. That is, we will want to allow the shared library code to set specific strategies either on a static module based manner or dynamically dependant on the specific routing configuration rules triggered by evidence meta data. Within the scope of our research project, lacking a router functionality, we opt to focus on distinguishing between modules with regard to posix\_fadvise strategy using a configuration file for our new CarvFS-like file-system. Additionally, while in a routing library based approach the use of \emph{readahead} may be warranted, within the scope of our research we shall focus on \emph{posix\_fadvise} and throttling. Given that some modules will look only at the file header, we shall distinguish between a head policy and a body policy. As a carvpath may be a sub entity of an other bigger entity, the current state of the parent region should also be relevant to our strategy for the sub entity. 

Other than the \emph{posix\_fadvice} policies, we also need to look at throttling. 
There are two types of throttling that are relevant. Throttling based on processing chunks of a large CarvPath entity that isn't marked  with posix\_fadvise in order to remain stored in the page-cache, and throttling with regards to the entering of fresh data into the growing archive of underlying data. Based on the information from \emph{/proc/meminfo} combined with the size of the currently mapped-as-active data chunks, two throttle levels can be calculated. One throttle level for CarvPath marking and one throttle level for archive expansion.  We shall be looking at different strategies for calculating these throttle levels once we have code to do so with. 
\section{Conclusions}
The use of the \emph{posix\_fadvise} API, especially the use of \emph{DONTNEED} to indicate a chunk of content no longer needs to reside in the page-cache, seems to be a pivotal concept within our project. 
The communication with the kernel gives us many hooks to implement a forensic framework in a way that allows us to minimize the amount of disk-cache misses in the system. As a result of multiple factors the interaction between the framework and the kernel page-cache mechanism will go through a CarvFS-like user space file-system and shared library code used by all individual modules within the framework. Our first proof of concept will be less configurable in an attempt to maximize disk-cache efficiency than would be possible, yet our projected file-system should implement all the needed interaction points to allow a future implementation of an advanced router library to implement more advanced tuning through interaction with the file-system. That is, the services for implementing this should all be there in the file-system implementation. The file-system will need to implement a type of CarvPath state graph that allows it to identify and eliminate data chunks that are no longer required to stay in the page-cache. It also needs to implement a way for a library to interact with it and set a policy or update the state for a specific CarvPath. Given the already identified dependance on the shared libraries, implementing throttling as a library poll loop seems reasonable for our proof of concept. We shall need to define a type of file-system based API, using either extended attributes or ioctl. As this will need to be combined with our desire to implement opportunistic hashing, the definition of this API will be the subject of a different appendix.
