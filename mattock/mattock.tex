\chapter{The Mattock Architecture}
This appendix tries to show the rough outlines of a possible future computer forensics framework that would built on the good parts and lessons learned from the Open Computer Forensics Architecture (OCFA), current day technological advancements, modern insights from the field of computer forensics and high-volume data processing, access controll technologies and the concepts and sub-systems introduced in the MatockFS dissertation that this document was written as an appendix for. 
The idea of this appendix is to draw a rough outline of a possible, partially OCFA inspired framework using modern day information technology components and insights gained from years of OCFA usage and the analysis done on OCFA timing that was described in Appendix-A. A framework that, if completed, could fill the gap that in an accademic sense was left by the discontinuation of OCFA development by the Dutch police. While this gap has been filled successfully for Dutch law enforcement by the Xiraf framework developed by the Netherlands Forensic Institute, and while on a smaller scale the framework provided by PyFlag and more notably the Sleuthkit Hadoop Framework have taken interesting steps, PyFlag, has not moved significantly beyond any of the same legacy implementation choices that OCFA made towards an architecture, to something more in sync with modern distributed computing insights. The Sleuthkit Hadoop Framework efforts on the other hand, like OCFA seem to have been abandoned. Neither architectures have addressed the basics of disk-cache efficiency as addressed in this paper. As far as a literature study has revealed, no open source frameworks that exists today has managed to provide the academic community with a framework suitable both for both small scale experiments and for the crucial experiments that work with a more sizeable lifelike data corpus and server cluster setup. Computer forensic research projects of the scale of for example the FIVES project that used OCFA at its core would hardly be possible today. 
This while OCFA in itself was actually never designed with an accademic setting in mind. The framework described in this appendix
aims to first and foremost address the needs of computer forensic accademic research, while keeping use as an alternate tool in cybercrime and other real world computer forensics investigations a serious option. 
While implementation of a full open source accademic forensic framework with the required properties falls far outside of the scope of a single M.Sc research project, this appendix will provide the architectural outline of such a framework and will explain how MatockFS would play a pivotal role in the realization of such a framework. We shall address the base architectural design proposed in this appendix as \emph{The Mattock Computer Forensic Framework}. The idea of the naming stems from the naming of the carving tool scalpel. The idea is that while a tool the size of and with the precision of a scalpel is indispensable at any scale, if you want to scale up your investigations to the scale of an archeological dig, you will need bigger tools like a mattock. Its just a working title for the prospect architecture. Anyone picking up on this paper is invited to come up with a more suitable name for the resulting framework. 
\section{The OCFA architecture}
\begin{figure}
\centering
\includegraphics[width=100mm]{mattock/libraryview.pdf}
\caption{The base OCFA architecture}
\label{fig:FlowInOut}
\end{figure}
When we look at the OCFA architecture, at its core we can clearly identify fully custom build asynchronous messaging framework for processing modules that communicate with a small set of networking components that act to combine the modules into dynamic chains of tools that are applied to parts of the forensic evidence data. The use of commodity software components is mostly limited to the pervasive use of XML technology and a central relational database. We shall now have a look at some of the core components of the OCFA framework.
\subsection{The AnyCast Relay and Persistent Priority Queue}
At its basis, OCFA was a message passing concurrency based system. One known issue with message passing concurrency is the use of buffers. While message passing environment like the Erlang programming language and platform opt to put producing processes to sleep when buffers fill up, OCFA opted for a different approach. In OCFA, the message passing buffers were managed by on-disk persistent priority queues. These queues only contained referenced to in database large text objects. The persistent queues were meant and designed to be fully crash resistant. The priority queues had a special \emph{'never'} priority to hold messages that were observed to crash specific modules. This allowed modules to be restarted and to skip problematic data until a maintenance programmer would look at the problematic data and buggy module to fix the problem and re-submit the messages in the never queue for further processing. The AnyCast relay was built on top of the persistent priority queue. Every module connected to the AnyCast relay and registered as a consumer of a certain type (a module \emph{instance}) and would go into a message processing event loop asking the AnyCast relay for new jobs. When a module was done with a piece of evidence data, or when a module derived a sub entity from such data (for example an attachment as child entity of an e-mail message), the module would send a message to the AnyCast Relay addressed at a special process named the \emph{router}. The AnyCast would keep track of irresponsive and broken network connections and would play an important role in having stale or crashed modules restarted in a way not unlike what is common practice in Erlang based architectures. On such a detected crash, messages that were still pending a response would be put aside in the never queue to be looked at by a technician at a later point in time. In OCFA the AnyCast relay served as a single server for all modules, independent of the server these modules would run on.
\subsection{OcfaLib, a domain specific asynchronous framework}
While today NodeJS has mainstreamed the idea of a generic asynchronous framework, and while in other programming languages generic asynchronous frameworks such as Twisted for Python or Boost::asio for C++ have been available for quite a while, OCFA was first built long before such systems became mainstream. As a result, OCFA basically ended up building its own asynchronous processing framework. We could say that OcfaLib, the C++ OCFA library was a domain specific asynchronous framework for use with the AnyCast server. 
\subsection{The legacy Module API}
OCFA came with two quite distinct module Application Programming Interfaces (APIs). This fact was the result of chronology of development. The first version of OCFA came with a module API not much unlike that of the current day Sleuthkit framework. A module would get a file to process and could add meta data to that file, or, when it wanted to for example mark an extracted e-mail attachment as child entity, would submit that information to the framework. The API consisted of a module initialization part and a single method called 'processEvidence' that a module was supposed to overload. From within processEvidence the module could either add meta or submit a child entity with added meta date.
\subsection{The Tree-graph API}
After new modules got added to OCFA, the legacy module API was found to be lacking in the meta-data area. The problem was that a module deriving a tree of children would not be able to set meta-data for deeper child entities. Only level zero and level one meta data was possible. As a result, the more powerful tree-graph API was added. As porting old modules to the new API was considered a waste of precious development time, the old API was also still continued after the introduction of the new API.
\subsection{The legacy CAS storage}
OCFA in its initial release came with a Content Address Storage system for storing data entities. Data was created or, lacking CarvPath facilities, first copied to a temporary file and hashed during copy. Once the hash was fully calculated, the temporary file was either moved to a location derived directly from the hash of its content, or discarded if an entity with the same hash was already present in the repository. 
\subsection{CarvFS}
Later releases of OCFA were made compatible with the use of CarvFS for parts of the storage needs. CarvFS integration has however remained a bit of a hack. The storage sub-system of OCFA used physical symbolic links to CarvFS CarvPaths inside of its primarily CAS based storage system. This meant that for example when using a CarvPath aware Sleuthkit MMLS module, storage of a partition in the OCFA CAS storage system required the full partition to be read for hashing purposes before it could be symlinked in the storage subsystem.
\subsection{The meta-data based message router}
At the core of the OCFA architecture was the central meta-data based router. This XML technology based router would parse the meta-data that modules had gathered on a data entity and would based on an also XML-based rule-list determine the next hop in the tool-chain for that data entity. 
\subsection{The use of an SQL server}
There were two technologies that were pervasively used within OCFA. One XML we already discussed. The second one was a relational database. It was used by the storage subsystem, by the messaging subsystem, and finally by the meta-data storing data-store-module. In each of these uses, the use of an SQL database turned out to be a sub-optimal choice for a number of reasons. Some related to the creation of run-time performance bottlenecks and others related to the nature of the data structure and the nature of useful analysis-time queries on this data. More on this when we discuss the alternatives for Mattock.
\section{PyFlag}
\begin{figure}
\centering
\includegraphics[width=50mm]{mattock/pyflag.jpg}
\caption{The base PyFlag architecture}
\label{fig:FlowInOut}
\end{figure}
Next to OCFA, the PyFlag framework deserves mention in this paper. While PyFlag has quite a different scope than OCFA it shares a lot of similarities too. Where OCFA is meant purely as a framework for computer forensics, PyFlag is a hybrid system that also addresses the field of network forensics. This hybrid approach makes that this framework would in a fundamental way be much more difficult to apply disk-cache related optimizations. It thus would be unfair to look at PyFlag purely from a large scale computer forensic data processing point of view in a comparative way. 
\section{The Sleuthkit Hadoop framework}
A more promising development was the Sleuthkit Hadoop framework. It aimed to combine The Sleuthkit with specific distributed technology. The technology proposed was a combination of the HDFS distributed file-system and the distributed NoSQL database HBASE, both part of the Hadoop technology stack. That is, a distributed file-system and NoSQL technology. We shall look in more detail at suitable NoSQL technology. While HBASE isn't a bad choice at the processing end, there are data structure and analysis concerns that would point to different NoSQL technology as potentially more suitable. Further, the \emph{initial read} performance of \emph{redundant} distributed storage might make looking closely at the performance/redundancy aspects for any distributed storage system. Today the concept of erasure encoding based distributed storage might provide an interesting middle ground between the two. Yet, taking a more NUMA or striping approach to scaling the storage for distributed computer forensic data processing might also proof an interesting alternative. 
\section{Non-open frameworks}
It is important to note that due to the closed nature of the frameworks involved, this paper does not look into some major and widely successful non-open forensic frameworks such as Xiraf or FTK Distributed Install. The scope of this appendix is limited to open tools and publications.
\section{Digital Evidence Bags}
So far we have been looking purely at scalability and performance of forensic architectures without considering other important new computer forensic insights. One conceptual idea that all current frameworks seem to discard are the key concept introduced with so called Sealed Digital Evidence Bags (S-DABs) by Bradley Schatz and Andrew Clark in 2006. While the details of S-DEBs fall outside of the scope of this appendix, one key aspect deserves special attention: The concept that both data and meta-data require a form of tamper-proofness. That is, once a piece of evidence data or meta-data is entered into the system, this (meta-)data should be considered to be logically immutable. We shall take this concept with us in our outlines for e next generation scalable framework.
\section{New insights}
Looking back at OCFA and other forensic frameworks, there are in retrospect important suboptimal choices that would be made differently if a framework like that was to be developed today. In this section we summarize a few important insights that arose from years long usage of OCFA, a survey of other open frameworks. A literature survey and the results of the timing analysis in the first appendix of this paper.
\begin{itemize}
\item \emph{A tree-graph API is essential} : While simpler API's can be useful for some trivial modules, all such modules could also work with a more generic tree graph API. Having just one generic tree-graph oriented API could facilitate a much wider range of modules and if the API is defined in asynchronous terms, the API could be portable to alternative architectures.
\item \emph{Leveraging asynchronous frameworks is essential} : OCFA implemented its own custom asynchronous framework and the part of the OCFA code-base involved with implementing that functionality was substantial. With current day asynchronous frameworks such as Boost::asio (C++), Twisted (Python) or NodeJS (JavaScript), the need for a custom built asynchronous framework has disappeared.
\item \emph{Evidence sealing, priviledge separation and access controll facilities are a must} : It is essential to limit the mutability time-span and scope to an absolute minimum. Both from an anti-forensics point of view, and from the point of view of the legal credibility of the integrity of the implemented forensic process.
\item \emph{Reducing disk-cache misses is essential for performance}: While many papers focus on CPU cycles being wasted by inefficient forensic processes, the truth is that much of the forensic process is IO rather than CPU constrained. As such, the fact that the same data \emph{will} get read multiple times should make clear that a disk-cache miss will impact throughput and that a forensic framework such as OCFA with a design that does not effectively mitigate disk-cache miss rates will suffer from throughput disk-cache-miss related IO bottlenecks.
\item \emph{Zero-storage carving is essential} : The process of locating, carving and validating files on disk images is complex and will either result in high false positive or high false negative counts. In the case of high false-positives, copy-out will result in massive needs for forensic archiving storage for derived entities. In the case of high false negatives essential data may be missed. Applying zero-storage carving facilities such as CarvFS will allow for a relatively low cost of false positives while minimizing the amount of additional storage required for processing.
\item \emph{Simple priorities don't really work} : While our research has shown that priority queuing as used in OCFA would be effective for homogeneously sized chunks of evidence data, not taking into account the size of the evidence entities and their likely disk-cache status in prioritizing has been shown to yield such poor results that the usefulness of priority queuing in such a way must be seriously questioned. 
\item \emph{Most current day forensic disk image formats are poorly suited for large-scale processing archives} : In large scale investigations encompassing dozens to hundreds of full-size disk images, the in-lab usage of the common computer forensic disk image storage formats (EFF \& AFF) have shown to be rather poorly suited due mostly to scalability issues of keeping hundreds of opened disk-image state-object open. Lacking a forensic-lab storage format for large archives of disk images, the use of simple sparse dd images currently seems to be better suited in a scalable lab envinronment than the direct usage of these formats. It would be good if future research would investigate the possibilities of archive friendly low system-footprint storage of a multitude of computer forensic data. 
\item \emph{Data migration should not be taken lightly} : While distributed file-systems or storage systems such as SNFS can facilitate in making evidence data available on many nodes, it is important to realize that accessing data from a different node will per definition result in a disk-cache miss on that node. It is suggested that data migration should prefer either the early out-of-cache migration of data that has not yet been fully cached by the originating module, or the migration of relatively small chunks of data targeted for relatively high-CPU processing on the other node. 
\item \emph{Relational (SQL) databases are a poor fit on most fronts} : In OCFA the Postgress SQL database was used for many things. In retrospect, certainly given the current day alternatives, these things would today all have better alternatives. First of all, the database usage for \emph{mutable} meta-data and for the storage and messaging subsystem together was a major performance bottleneck. Apart from the fact that in retrospect in-process mutable meta-data does not fit in with the SDEB view of things, a relational database is a poor choice of technology for implementing either such meta-data \emph{document} storage or the extra indirections implemented within the storage and messaging subsystems. More than that though, the usage of an SQL database by the data storage module and the user interface have shown that many of the more advanced analysis's have had such shape and form that the database and queries would have been much better of having a more \emph{graph} oriented infrastructure. If we look at OCFA and than look at modern NoSQL technology, we see that parts of the SQL functionality had better be implemented without a database, some had better be implemented with a distributed \emph{key/value store} database, some with a \emph{document} database and some with a \emph{graph} database.  One thing all aspects of OCFA have in common is that SQL technology in todays technology landscape would be a sub-optimal choice at best. 
\item \emph{Kick-starting may not start at a server node} : In OCFA kick-starting took place from a server node. Before this could be done however, the EWF files had to be placed on an SNFS partition accessible to the server from a client through an SNFS connected file-server. Combine this with the need for converting EWF to dd or an other lab friendly format, the lack of a client-based EWF submitter led to massive disk write inefficiency. A kick-starting network client would need to be an essential component in a modern forensic framework.
\item \emph{Not all modules are alike} : Todays open source computer forensic frameworks treat modules as equal citizens. The reality however is that some modules such as a file-type module are so common in data processing that framework embedding would be justified, some modules such as simple carvers are used mostly on huge data files and are IO intensive while others like OCR work on relatively small data chunks and take up significant CPU resources. Treating all modules and all data as similar will inevitably lead to poor overall framework performance. 
\item \emph{Globally valid annotations are the key} : As each server in a forensic data processing cluster has its own disk cache, the concept of dividing the load between different nodes by distributing and redistributing to different nodes should be positively influenceable by allowing nodes to have a common communicable notion of the portions of the global investigation data they and other nodes likely still have in their cache. For this, a globally valid annotation for data chunks is essential CarvPath annotations could play a major role in this.
\item \emph{Meta-data serialization technology matters} : At the time that OCFA was devised, XML was the only logical choice for meta-data serialization. The XML technology stack though is far from being the most efficient serialization form for forensic meta data. While there are multiple papers proposing standardized XML formats for forensic meta-data exchange, the inefficiency and resource requirements of XML processing forms a major bottleneck in event-rich high throughput processing envinronments such as within a computer forensic framework. More efficient serialization options such as JSON, Protocol Buffers or Cap'n Proto should be seriously considered as alternative to XML. 
\item \emph{Hashing algorithm choice is essential} :  Traditionally MD5 and later also SHA1 have been used as hashing algorithms in digital forensics. From a cryptographic point of view, MD5 is now deprecated while SHA1 is in the process soon becoming deprecated. Many public and law-enforcement-only hash collections such as for example the Virusshare hash set or national child pornography hash sets are still distributed only with MD5 and/or SHA1 hashes. Others like the NIST NSRL are now supplemented with SHA256 hashes. While the later may sound like good news, there is an other important issue with SHA256 for use in a forensic framework: SHA256 may be significantly more secure as a hashing algorithm than SHA1 or MD5, it is also significantly more CPU intensive. So much so that it may undermine the whole concept of opportunistic hashing as presented in this paper. While today NIST still considers the use of SHA1 for purposes as defined in this paper as \emph{acceptable use}, we must consider the retroactive impact that a cryptographer testifying on behalf of the defense and questioning the continued use of (the no longer collission resistent) SHA1 in the forensic process may have a few years from now. It thus can be argued that moving forward to a non-deprecated secure hashing algorithm should be considered a priority. Given the CPU resource issues with SHA256, it is also of paramount significance that the algorithm we move forwards to should have at least reasonable resource requirements in its software implementation. A quick study into available secure hashing algorithms reveals a small family of secure hashing algorithms. SKEIN, BLAKE and BLAKE2 share properties that would make each of these secure hashing algorithms prime candidates for supplementing and eventually replacing SHA1 as primary hashing algorithm for computer forensics. In the next appendix we will make the case for one of these. The important insight here however is the notion that SHA1 is close to deprecation, SHA256 puts to much strains on resources in a high performance computer forensics setup and we need to pick a more suitable replacement.     
\end{itemize}
\section{A modernized OCFA inspired open-source architecture}
With the good parts from the old OCFA architecture combined with the new insights above, we can sketch the base outlines of a next generation message passing concurrency open computer forensic framework. Let us start out with a rough description of the changes from the OCFA architecture to our new Mattock architecture outline. The most obvious change can be seen in the dependencies. We no longer have a central SQL server dependency.  We see that the custom asynchronous framework gets replaced with a standard asynchonous framework. This could be boost::asio for C++ or for example twisted for the Python language. We see that file-type logic and meta-data router are no longer a separate module and network service but are now integrated in the base functionality for each module process. As in OCFA, a module instance runs in its own process as part of the asynchonous framework that it runs under. One module instance per async-framework process and possibly more module instances per module type if the server architecture and module type warant multiple instances of the same module to be run on the same machine.  These could be CPU intensive modules on a server with a high amounth of cores. 
We see that the number of module APIs reduced to just the tree-graph API.  The most obviously notable change however is the fact that there is no longer a direct dependency between generic modules and database technology. The SQL server has disappeared and has been replaced by other technology. The Data Store Module (DSM) maps the meta-data into some NoSQL database. This will most likely be a distributed graph-database or a distributed document-database (or possibly a hybrid combination of the two such as ArangoDb). Both data and now also meta-data are stored in a write-once way to the CarvFS replacement MattockFS that implements a subset of the SDEB concept by means of privilege separation and immutability. The CarvFS long-path SQL database is replaced with a distributed key/value NoSQL database. While MattockFS should initially still run on something like SNFS, the use of a distributed file-system should be a potential way forward to further improve scalability. The AnyCast Relay functionality has mostly moved to a system bus like facility provided by the MattockFS filesystem, combined with a monitoring and load balancing meshup that connects the anycast functionality of all Mattock server nodes and will migrate jobs to other nodes when and only when the CPU needs for a job are expected to outweigh the cost of the page-cache miss on the peer node. One node per worker host. The kick-starting process is revised. EWF processing is moved to the client side of things. The client connects to a central kick balancer that will query all AnyCast monitors to find the right kick server to redirect the client to. 
\begin{figure}
\centering
\includegraphics[width=100mm]{mattock/libraryviewmattock.pdf}
\caption{The base Mattock architecture}
\label{fig:FlowInOut}
\end{figure}
\subsection{The distributed long-path store}
The use of CarvPath annotations works very well for most data entities originating from digital media. In some cases, such as for log files on bussy servers or large files downloaded in paralel over slow P2P connections, the files can be so fragmented that the carvpath ends up being longer than what would be usefull as a file or directory name in a file-system. To overcome that problem, long carvpath annotations are replaced with a secure hash of the actual carvpath. Given that storage tokens should be the same on each server node, knowledge to resolve these short notations back to their full carvpath needs to be distributed. Given that the data stored are just simple key/value pairs, we propose that a NoSQL key/value store database technology would be most suitable to fill this task. A solution like Redis could be a viable implementation, but concideration regarding the speed versus persistence trade-off is important.
\subsection{SNFS vs distributed storage}
OCFA as used in its setup at the dutch national police made use of SNFS on shared central block storage over fiber-channel. Alternative setups used a combination of local storage with NFS. It makes sense to set up a cluster of servers in such a way that kickstarting goes to a piece of local storage that is available on all other nodes using NFS mounts. A solution like that is basically the simplest form of non-redundant distributed storage. Mattock should work on these solutions or on any distributed storage system that allows for the creation of and efficient random access to relatively large \emph{sparse} data files. Distributed storage that emphesizes redundancy over performance might pose a problem, yet today technologies such as erasure encoding should allow for RAID-like properties for distributed storage that for larger numbers of nodes combine good performance with decent redundancy. Futher study into suitable distributed storage solutions seems appropriate.
\subsection{MattockFS}
MattockFS is discussed in full detail in the main paper. It is a user-space file-system that is centered around CarvPath anotations and entities that consist of fragments and/or sparse regions. MattockFS runs on top of a distributed storage solution as described above and uses the distributed long-paht store to store longer carvpath annotations and make these work transparently on other nodes. The file-system provides the different async-modules with a file-system based API to storage and messaging. Appart from the carvpath based data access, the MattockFS file-system based API provides:
\begin{itemize}
\item Immutability: The file-system provides facilities to create entities that are mutable only on creation.
\item CarvPath based batches that aim to provide for opportunistic hashes. A CarvPath can be marked as a batch and as it (and possibly its (ancestors or decendants) passes multiple modules before the batch is decomissioned, a hash may be calculated opportunistically.
\item Batch based fadvice: as batches get created and decomissioned, MattockFS can communicate with the kernel about chunks of repository data being potentially needed or will never get read again. The kernel should be able to use this data to become smarter with respect to page-cache usage, thus reducing the potential for page-cache miss performance issues.
\item Throthling: As the file-system keeps track of all active batches and notifies the kernel about data chunks no longer needed in the page-cache, MattockFS can provide statistics to async-modules that act as throthling advice for these modules. It provides total batches-size information and provides a \emph{what-if} facility allowing a module to query how much a new CarvPath would grow the active batches volume. An async-module should use this info to temporarily restrain from entering new (derived) data or uncached CarvPaths into the system untill the point when the total amounth of active batch space has gone down. 
\item Anycast inter-process data exchange: Using extended attributes, symlinks and special directories, the file-system provides the possibility to mark data with a \emph{next} marker, placing it in an AnyCast like queue. It also allows to accept data for processing as a module.
\item Sparse capabilities: For processing a job and for one-time mutable access to a newly created data entity, the file-system provides sparse capabilities. These are basically one-time passwords for small chunks of authority. one-time passwords that can however be communicated between processes so that tool invocations by modules remains a possibility.
\item Total page-cache pressure: Provides the anycast monitor with info needed to potentially migrate jobs to other nodes.
\end{itemize}
\subsection{The AnyCast Monitor mesh-up}
Migrating a batch to an other node is expensive. Any opportunistic hashing will need to start from nothing, data won't exist in page cache on the other node, and unless the storage was highly redundant is the extra networking overhead of making the data available at all on the other node. For CPU intensive modules however, migrating a job might be well worth the cost. An Anycast monitor process tries to keep track of things. The amounth of page-cache potentially used up by active batches, the overall system load, the CPU and RAM usage of different worker modules, etc. This info is than all shared with AnyCast monitors on other server nodes. If a large inbalance is detected between node's, an AnyCast monitor may act as one or more modules and accept jobs and close their coresponding batches on the source server node. These jobs are than recreated by a monitor on a different node where they should normaly live to complete the rest of the batch.
\subsection{Client/Server kick-starting}
\subsection{Store-system embedded lib-magic functionality}
\subsection{Distributed routing logic}
\subsection{From SQL to Graph based NoSQL}
\subsection{Using language-native asynchronous frameworks}
\subsection{A single tree-graph, lambda and asynchronous operation oriented API}
\section{Possible Mattock subjects for dissertations}
\begin{figure}
\centering
\includegraphics[width=100mm]{mattock/future.pdf}
\caption{The base Mattock architecture}
\label{fig:FlowInOut}
\end{figure}
\subsection{MattockFS related subjects}
\subsubsection{Persistent Anycast}
\subsubsection{Advanced algoritms}
\subsubsection{MattockFS in C++}
\subsubsection{Hardening}
\subsubsection{Composition \& Storage}
\subsubsection{Distributed storage}
\subsection{Kickstart}
\subsection{Framework related subjects}
\subsubsection{Asynchonic framework \& treegraph API}
\subsubsection{Serialisation}
\subsubsection{Routing pollicies}
\subsubsection{Asynchonic framework \& C++}
\subsection{Module related subjects}
\subsubsection{Module regression testing}
\subsubsection{Core libtsk modules}
\subsubsection{DSM \& NoSQL}
\subsubsection{Text extraction and indexing}
\subsubsection{Pdf \& OCR}
\subsection{Secure hashing}
\section{Conclusion}


