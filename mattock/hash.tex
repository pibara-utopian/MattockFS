\chapter{Algorithms for opportunistic hashing}
Traditionally digital forensics has always used secure hashes for multiple purposes. With new cryptographic advances in the subfield of secure hashes, the specific algoritmhs that are commonly used in digital forensics have come to be considered cryptographically deprecated. This appendix looks into these deprication issues with commonly used algoritms, compatibility issues with moving to other algoritms and performance and scalability considerations with alternative secure hashing functions.

\section{Hashing algorithm choice is essential} 
Traditionally MD5 and later also SHA1 have been used as hashing algorithms in digital forensics. From a cryptographic point of view, MD5 is now deprecated while SHA1 is in the process of being deprecated. Many public and law-enforcement-only hash collections like the Virusshare hash set or national child pornography hash sets are still distributed only with MD5 and/or SHA1 hashes. Others like the NIST NSRL contain MD5 and SHA1 hashes but are now supplemented with SHA256 hashes. While the later may sound like good news, there is an other important issue with SHA256 for use in a forensic framework: SHA256 may be significantly more secure as a hashing algorithm than SHA1 or MD5, it is also significantly more CPU intensive and not parallelizable. These properties may undermine the whole concept of opportunistic hashing as presented in this paper. While today NIST still considers the use of SHA1 for purposes as defined in this paper as \emph{acceptable use}, we must consider the retroactive impact that a cryptographer testifying on behalf of the defense and questioning the use of SHA1 in the forensic process may have a few years from now if SHA1 reaches the same level of deprecation that MD5 has today. Signs that SHA1s practical deprication is near are plentyfull. Recently research showed that the use of GPUs can be effectively leveraged significantly reduce the cost of generating a SHA1 colission. While the specific purposes for what hasing is used within the computer forensics process for a great part may still safely use SHA1 or even MD5 today from a technical and ecconomical perspective, the potential impact of an eloquent expert testifying on behalf of the defense against the use of such deprecated hashes must not be underestimated. It thus can be argued that moving forward to a non-deprecated secure hashing algorithm should be considered a priority for digital forensics.  Given the CPU resource issues with SHA256, it is also of paramount significance that the algorithm we move forwards to should have at least reasonable resource requirements in its software implementation. A quick study into available secure hashing algorithms reveals a small family of secure hashing algorithms share properties that would make each of these secure hashing algorithms prime candidates for supplementing and eventually replacing SHA1 as primary hashing algorithm for computer forensics.

\section{Deprecation}
In this section we will look at why MD5 and SHA1 should be considered cryptographically deprecated and while this has limited impact on the technological and ecconomic realities of digital forensics, it may pose a serious problem in the legal arena.
\subsection{Collision resistance}
When looking into the deprication of SHA1 and MD5, the sole reason for practical deprication of these two algoritms stems from attacks that reduce the collision resistance. Colission resistance comes in multiple forms, but the base property lies in the dificulty of finding two data entities with the exact same hash in a situation where both data entities may be manipulated. In its simplest form such collission would be fixed size relatively small chunks of data, but in a more complex form colissions may be generated by manipulating just the last part of larger data entities. Given the use of whitelists with hashes of files that are part of known software packages, a collission like this might be used as anti-forensic technique by for example generating a colission between a modified version of a clipart image and an image containing illegal content. If the clipart can than be made part of some whitelistable collection, the illegal content shall remain under the radar indefinetely. A similar attack may be possible without poisoning the whitelists. A computer forensic framework may opt not to process the same entity twice and may use a secure-hash to stop itself from doing so. If a subject can poison the \emph{already-processed} hash collection, than a second file with different content may be ommitted from being processed later on. These attacks are technically feasible with MD5 and should be theoretically achievable with SHA1. From a practical point of view though, the use of cryptographic containers that support the concept of plausibly denyable encryption would seem a much more ecconomical way to achieve the same data hiding goals. There currently are no concrete indications that collision attacks might be in use as anti-forensic data-hiding technique. 
\subsection{First pre-image resistance}
Other than colission resistance, in pre-image resistance lies in the dificulty of finding a data entity that, when hashed, yields a given hash. If a hash function is colission resistance, than it should be pre-image risistant as well. An ecconomical pre-image attack could allow simpler anti-forensic data-hiding. Colissions could be created with any hash from the commonly used white lists in order to hide data or a denial of service against known bad could be possible. Imagine a large set of photo's and other images being modified to match hashes from a child pornograph related database. A first pre-image attack agains a hashing function would have a significant impact on the use in forensic frameworks. One possibly even worse problem with a first pre-image attack possibility would be the the legal ramifications of the posibilities of investigators to inject fabricated evidence into a disk image while maintaining the validity of the recorded hash. Digital forensic good-practice dictates that a secure hash is generated and recorded out-of-band when a disk image is created. This practice ensures that lab-investigators can't accidentaly or out of malice change the content of the disk image. A pre-image attack could allow an investigator to inject false evidence into a disk image file without changing the validity of the secure hash that was generated during acquisition. 
\subsection{First pre-image resistance and hash sets}
Combined hash-sets used in digital forensics can grow relatively big. As a result an anti-forensic pre-image attack could become somewhat more viable resulting from the fact that a typical hash collection for combined black and white listing will typically have \(2^{26} .. 2^{28} \) hashes. This fact makes it feasible that a first pre-image attack that isn't practically feasible against a single target hash, may be feasible when implemented as \(2^{28}\) paralel attacks. 
\subsection{Use in forensic frameworks}
Given that SHA1 and MD5 due to their deminished colission resistance should be considered deprecated from a colission resistance point of view does not imply that from a technical point of view they are now unusable for all computer forensic purposes. The combination however of deminished colission resistance with the properties of the hash set sizes used in the forensic process should however be sufficient to cast a shadow of doubt on the use of these deprecated algoritms in a forensic framework. While today anti-forensic attacks may be unseen and unpracticle, the continued use of these algoritms in combinationwith sizeable list of process flow influencing hash lists seems a fundamentally flawed approach waiting for a serious attack fector to happen.
\subsection{Use for guarding the forensic process}
While there seems no technical ground for discontinuing the use of first pre-image resistant hashes for guarding the forensic process as done by the out-of-band recording of hashes at aquisition time, we must remember that a judge is bound to lack the deep level of technical insight needed to distinguish between colission resistance related deprication of a hash, possible large-collection first-pre-image related issues, and first-pre-image related issues. Given this reality, it would be quite easy for a judge to get confused between the responsible use of for example SHA1 for safeguarding the integrity of a full disk image, a use that should only suffer when first pre-image resistance is significantly reduced, the use where a \(2^{28}\) entry large set of hashes would make a less significantly reduced first pre-image resistance a serious problem and the use where even the reduced colission resistance could pose a problem. Given this likely confusion, it should be considered a real posibility that a judge, using a cryptographic expert to gain some understanding of the weaknesses of a hashing algoritm might for exapme dismiss digital forensic findings based on the fact that the digital forensic investigators knowingly used deprecated hashing algoritms and compromized the esential integrity of the forensic process by not using any of the available modern secure hashes. One additional problem with a judge comming to such a conclusion is that legel rulings can have ramifications that stretch well beyond the confines of just the specific legal case. While on technical and economical grounds the continued use of cryptographically broken hashes may be defensible, from a legal risk perspective for the procecution, moving forward seems even more urgent as it does from a technical perspective. 
\subsection{MD5}

\subsection{SHA1}
\subsubsection{Freestart}
\section{Alternatives}
\subsection{SHA256}

\subsection{SKEIN, BLAKE \&BLAKE2}

\section{Choice of algorithm}
If we discard the interesting and important efforts in the fields of partial and fuzzy hashing and focus on full-entity hashing, there are four main ways how hashes are used in computer forensic processing.
\begin{itemize}
\item Check against known good. This includes abandoning further processing for for example Windows OS system files.
\item Check against known bad. This includes marking for example known child pornography image files.
\item Check against \emph{seen before}. This can keep a forensic framework from for example unzipping a large archive that is found in many places for every place where it was found.
\item Applying set-theory to groups of files from different sources within a single investigation. This includes things like allowing an investigator to select all office documents unique to the intersection of office files found on the systems of two suspects.
\end{itemize}
For the first two purposes, compatibility of the hashing algorithms used in the known file hashes data set with the hashing algorithm used by the framework is important. For the other two purposes there exists only one algorithm; the one used by the framework. It is with the first two types of usage that we run into an issue witch choosing a hashing algorithm for MattockFS. The problem is that data-set producers seem to be behind the curve with regards to secure hashing. Many known-file data-set providers only provide SHA1 hashes. According to NIST, SHA1 is deprecated for the \emph{generation} of secure hashes (as was MD5 before it). Even NIST though still distributes their \emph{NSRL} known-file hash data-set with (next to SHA256 hashes) SHA1 hashes.  The apparent reason for this is that the secure SHA256 hash has very poor performance when used in software. As use of hashes in computer forensics involves the hashing of large amounts of data, and as access speeds to that data are increasing as the use of solid state disks in CF setups gains more traction, the poor performance of the more secure SHA256 becomes a serious performance consideration. There are several alternative hashing algorithms that combine a strong and secure hash with a decent or good performance on a software platform. Most notably two (non-winning) candidates for SHA-3: SKEIN and BLAKE. Those however come with a lack of compatibility with our dataset providers. Finally, a successor to BLAKE, BLAKE2bp comes in forms optimized to take advantage of 64 bit multi core systems. We must acknowledge though that SKEIN and the original BLAKE, as being part of the SHA-3 competition will likely have been more scrutinized than BLAKE2, so our trust may be slightly less in that algorithm. A short overview :
\begin{table}[]
\centering
\begin{tabular}{llllr}
Algorithm & Secure & Trusted & Compatible & Speed compared to SHA1 \\ \hline
SHA1 & NO & YES & YES & 100\% \\
SHA256 & YES & YES & YES & 30\% \\
SKEIN & YES & YES & NO & 75\% \\
BLAKE2b & YES & NO* & NO & 135\% \\
BLAKE2bp & YES & NO* & NO & 420\% \\
\end{tabular}
\end{table}
When we look at the numbers above, we can conclude that combining SHA1 with the multi-core 64 bit optimized BLAKE2bp algorithm will give us at least the security of SHA256 combined with the compatibility of SHA1 at a performance hit of only 20\% when compared to only using SHA1. We pose that a final version of MattockFS shall support the following modes and algorithms for the user to choose between:
\begin{itemize}
\item SHA1 only (100\% speed; compatible/insecure)
\item BLAKE2bp only (420\% speed; incompatible/secure)
\item Dual hashing (80\% speed; compatible/secure)
\end{itemize}
Within the context of the initial version of MattockFS that falls within the scope of this research project, we shall implement only one of these modes.
\section{Conclusion}

