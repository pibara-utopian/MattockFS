\chapter{Algorithms for opportunistic hashing}
Traditionally digital forensics has always used secure hashes for multiple purposes. With new cryptographic advances in the subfield of secure hashes, the specific algorithms that are commonly used in digital forensics have come to be considered cryptographically deprecated. This appendix looks into these deprecation issues with commonly used algorithms, compatibility issues with moving to other algorithms and performance and scalability considerations with alternative secure hashing functions.
It also adresses how opportunistic-hashing geared algortitms in MattockFS further underline the need for specific hash function properties. 
\section{Hashing algorithm choice is essential} 
Traditionally MD5 and later also SHA1 have been used as hashing algorithms in digital forensics. From a cryptographic point of view, MD5 is now deprecated while SHA1 is in the process of being deprecated. Many public and law-enforcement-only hash collections like the Virusshare hash set or national child pornography hash sets are still distributed only with MD5 and/or SHA1 hashes. Others like the NIST NSRL contain MD5 and SHA1 hashes but are now supplemented with SHA256 hashes. While the later may sound like good news, there is an other important issue with SHA256 for use in a forensic framework: SHA256 may be significantly more secure as a hashing algorithm than SHA1 or MD5, it is also significantly more CPU intensive and not parallelizable. These properties may undermine the whole concept of opportunistic hashing as presented in this paper. While today NIST still considers the use of SHA1 for purposes as defined in this paper as \emph{acceptable use}, we must consider the retroactive impact that a cryptographer testifying on behalf of the defense and questioning the use of SHA1 in the forensic process may have a few years from now if SHA1 reaches the same level of deprecation that MD5 has today. Signs that SHA1s practical deprecation is near are plenty-full. Recently research showed that the use of GPUs can be effectively leveraged significantly reduce the cost of generating a SHA1 collision. While the specific purposes for what hashing is used within the computer forensics process for a great part may still safely use SHA1 or even MD5 today from a technical and economical perspective, the potential impact of an eloquent expert testifying on behalf of the defense against the use of such deprecated hashes must not be underestimated. It thus can be argued that moving forward to a non-deprecated secure hashing algorithm should be considered a priority for digital forensics.  Given the CPU resource issues with SHA256, it is also of paramount significance that the algorithm we move forwards to should have at least reasonable resource requirements in its software implementation. A quick study into available secure hashing algorithms reveals a small family of secure hashing algorithms share properties that would make each of these secure hashing algorithms prime candidates for supplementing and eventually replacing SHA1 as primary hashing algorithm for computer forensics.

\section{Deprecation}
In this section we will look at why MD5 and SHA1 should be considered cryptographically deprecated and while this has limited impact on the technological and economic realities of digital forensics, it may pose a serious problem in the legal arena.
\subsection{Collision resistance}
When looking into the deprecation of SHA1 and MD5, the sole reason for practical deprecation of these two algorithms stems from attacks that reduce the collision resistance. Collision resistance comes in multiple forms, but the base property lies in the difficulty of finding two data entities with the exact same hash in a situation where both data entities may be manipulated. In its simplest form such collision would be fixed size relatively small chunks of data, but in a more complex form collisions may be generated by manipulating just the last part of larger data entities. Given the use of white-lists with hashes of files that are part of known software packages, a collision like this might be used as anti-forensic technique by for example generating a collision between a modified version of a clip-art image and an image containing illegal content. If the clip-art can than be made part of some whitelistable collection, the illegal content shall remain under the radar indefinitely. A similar attack may be possible without poisoning the white-lists. A computer forensic framework may opt not to process the same entity twice and may use a secure-hash to stop itself from doing so. If a subject can poison the \emph{already-processed} hash collection, than a second file with different content may be committed from being processed later on. These attacks are technically feasible with MD5 and should be theoretically achievable with SHA1. From a practical point of view though, the use of cryptographic containers that support the concept of plausibly deniable encryption would seem a much more economical way to achieve the same data hiding goals. There currently are no concrete indications that collision attacks might be in use as anti-forensic data-hiding technique. 
\subsection{First pre-image resistance}
Other than collision resistance, in pre-image resistance lies in the difficulty of finding a data entity that, when hashed, yields a given hash. If a hash function is collision resistance, than it should be pre-image resistant as well. An economical pre-image attack could allow simpler anti-forensic data-hiding. Collisions could be created with any hash from the commonly used white lists in order to hide data or a denial of service against known bad could be possible. Imagine a large set of photo's and other images being modified to match hashes from a child pornography related database. A first pre-image attack against a hashing function would have a significant impact on the use in forensic frameworks. One possibly even worse problem with a first pre-image attack possibility would be the the legal ramifications of the possibilities of investigators to inject fabricated evidence into a disk image while maintaining the validity of the recorded hash. Digital forensic good-practice dictates that a secure hash is generated and recorded out-of-band when a disk image is created. This practice ensures that lab-investigators can't accidentally or out of malice change the content of the disk image. A pre-image attack could allow an investigator to inject false evidence into a disk image file without changing the validity of the secure hash that was generated during acquisition. 
\subsection{First pre-image resistance and hash sets}
Combined hash-sets used in digital forensics can grow relatively big. As a result an anti-forensic pre-image attack could become somewhat more viable resulting from the fact that a typical hash collection for combined black and white listing will typically have \(2^{26} .. 2^{28} \) hashes. This fact makes it feasible that a first pre-image attack that isn't practically feasible against a single target hash, may be feasible when implemented as \(2^{28}\) parallel attacks. 
\subsection{Use in forensic frameworks}
Given that SHA1 and MD5 due to their diminished collision resistance should be considered deprecated from a collision resistance point of view does not imply that from a technical point of view they are now unusable for all computer forensic purposes. The combination however of diminished collision resistance with the properties of the hash set sizes used in the forensic process should however be sufficient to cast a shadow of doubt on the use of these deprecated algorithms in a forensic framework. While today anti-forensic attacks may be unseen and impractical, the continued use of these algorithms in combination with sizeable list of process flow influencing hash lists seems a fundamentally flawed approach waiting for a serious attack factor to happen.
\subsection{Use for guarding the forensic process}
While there seems no technical ground \emph{yet} for discontinuing the use of first pre-image resistant hashes for guarding the forensic process as done by the out-of-band recording of hashes at acquisition time, we must remember that a judge is bound to lack the deep level of technical insight needed to distinguish between collision resistance related deprecation of a hash, possible large-collection first-pre-image related issues, and first-pre-image related issues. Given this reality, it would be quite easy for a judge to get confused between the responsible use of for example SHA1 for safeguarding the integrity of a full disk image, a use that should only suffer when first pre-image resistance is significantly reduced, the use where a \(2^{28}\) entry large set of hashes would make a less significantly reduced first pre-image resistance a serious problem and the use where even the reduced collision resistance could pose a problem. Given this likely confusion, it should be considered a real possibility that a judge, using a cryptographic expert to gain some understanding of the weaknesses of a hashing algorithm might for example dismiss digital forensic findings based on the fact that the digital forensic investigators knowingly used deprecated hashing algorithms and compromised the essential integrity of the forensic process by not using any of the available modern secure hashes. One additional problem with a judge coming to such a conclusion is that legal rulings can have ramifications that stretch well beyond the confines of just the specific legal case. While on technical and economical grounds the continued use of cryptographically broken hashes may be defensible, from a legal risk perspective for the prosecution, moving forward seems even more urgent as it does from a technical perspective. 
\subsection{MD5}
MD5 is a secure hashing algorithm that has been in wide use in digital forensics and is still used as sole hashing algorithm for for example the Virusshare hash set. In 2004 Xiaoyun Wang and Hongbo Yu found that MD5 is not collision resistant. Today MD5 collisions are not just possible but practical to the extent that using commodity hardware, generating multiple collisions per second has become a reality. In 2009 Yu Sasaki, Kazumaro Aoki found a pre-image attack against MD5 with a complexity of \(2^{123.4}\). It is generally accepted that MD5 is deprecated as a secure hashing algorithm and should not be used, even though the pre-image attack may not be of practically usable complexity.
\subsection{SHA1}
When MD5 started showing major problems, digital forensics started to largely move towards the use of SHA1. The performance of SHA1 does not deviate significantly from that of MD5. In 2005 Wang, Yin \& Yu reportedly showed the possibility of a collision attack with a complexity of \(2^{69}\) in a limited circulated paper. Today, in 2015 Stevens, Karpman \& Peyrin demonstrated a practical collision using ten days and a 64 GPU cluster. In cryptographic circles SHA-1 is widely considered as deprecated and Stevens, Karpman \& Peyrin's work on practical collisions supports this move. At the moment of writing NIST still addresses the use of SHA1 for the computer forensic purposes of known-good/known-bad matching. These statements however predate the practical collision. Accelerated deprecation of SHA1 in the wake of this practical collision would be in line with expectations.
\section{Alternatives}
While there is sufficient reason to avoid the use of MD5, and plan for the eminent wider deprecation of SHA-1, the path forward is not directly evident. NIST includes SHA256 hashes in their hash sets, yet SHA256 may not be the most suitable for use with digital forensic processes. We look at different alternative hashes in order to pick.
\subsection{SHA256}
SHA256 is one of the hash functions from the SHA2 family. While NIST has added SHA256 to its NSRL hash lists, there are reasons why SHA256 may not be the most logical step forward for extensive use in a forensic framework. While NIST's choice for SHA256 and lack of concrete plans for adding other hashes to their datasets would make SHA256 a good choice from a compatibility point of view, the question of performance and system resource usage raises serious questions about its usability. While in the past hashing overhead may have been negligible due to IO overhead and throughput speeds, in modern system setups IO latency and throughput have changed the balance and choosing a hashing function that triples hashing overhead has become more of an obstacle than it would have been in the past.  
\subsection{SKEIN, BLAKE \& BLAKE2}
While SHA2 was meant to succeed SHA1, SHA2 itself has now been succeeded by SHA3. The SHA3 standard was the result of a competition by NIST to find a successor for SHA2, as it was feared that SHA2 might soon proof to be broken in a way similar to SHA1. Where SHA3 aimed to provide a general purpose hashing function, the needs for digital forensics are more specific geared toward software implementations and variable length data. If we look at the SHA3 candidates and focus purely on the selection criteria that would make these candidates suitable for digital forensics, than the final winner (Keccak) would not have been the best candidate for a secure hash replacement for SHA1 in the field of digital forensics. There were however two SHA3 candidates, SKEIN and BLAKE that showed software implementation properties that make these algorithms more likely candidates for an acceptable SHA1 replacement. Further, there has been some further development on an algorithm that must be seen as part of the same family as SKEIN and BLAKE. BLAKE2 is a continuation of the BLAKE efforts that is even significantly faster than SHA1 on modern server architectures.
\section{Choice of algorithm}
If we discard the interesting and important efforts in the fields of partial and fuzzy hashing and focus on full-entity hashing, there are four main ways how hashes are used in computer forensic processing.
\begin{itemize}
\item Check against known good. This includes abandoning further processing for for example Windows OS system files.
\item Check against known bad. This includes marking for example known child pornography image files.
\item Check against \emph{seen before}. This can keep a forensic framework from for example unzipping a large archive that is found in many places for every place where it was found.
\item Applying set-theory to groups of files from different sources within a single investigation. This includes things like allowing an investigator to select all office documents unique to the intersection of office files found on the systems of two suspects.
\end{itemize}
For the first two purposes, compatibility of the hashing algorithms used in the known file hashes data set with the hashing algorithm used by the framework is important. For the other two purposes there exists only one algorithm; the one used by the framework. It is with the first two types of usage that we run into an issue witch choosing a hashing algorithm for MattockFS. The problem is that data-set producers seem to be behind the curve with regards to secure hashing. Many known-file data-set providers only provide SHA1 hashes. According to NIST, SHA1 is deprecated for the \emph{generation} of secure hashes (as was MD5 before it). Even NIST though still distributes their \emph{NSRL} known-file hash data-set with (next to SHA256 hashes) SHA1 hashes.  The apparent reason for this is that the secure SHA256 hash has very poor performance when used in software. As use of hashes in computer forensics involves the hashing of large amounts of data, and as access speeds to that data are increasing as the use of solid state disks in CF setups gains more traction, the poor performance of the more secure SHA256 becomes a serious performance consideration. There are several alternative hashing algorithms that combine a strong and secure hash with a decent or good performance on a software platform. Most notably two (non-winning) candidates for SHA-3: SKEIN and BLAKE. Those however come with a lack of compatibility with our dataset providers. Finally, a successor to BLAKE, BLAKE2bp comes in forms optimized to take advantage of 64 bit multi core systems. We must acknowledge though that SKEIN and the original BLAKE, as being part of the SHA-3 competition will likely have been more scrutinized than BLAKE2, so our trust may be slightly less in that algorithm. A short overview :
\begin{table}[]
\centering
\begin{tabular}{llllr}
Algorithm & Secure & Trusted & Compatible & Speed compared to SHA1 \\ \hline
SHA1 & NO & YES & YES & 100\% \\
SHA256 & YES & YES & YES & 30\% \\
SKEIN & YES & YES & NO & 75\% \\
BLAKE2b & YES & NO* & NO & 135\% \\
BLAKE2bp & YES & NO* & NO & 420\% \\
\end{tabular}
\end{table}
When we look at the numbers above, we can conclude that combining SHA1 with the multi-core 64 bit optimized BLAKE2bp algorithm will give us at least the security of SHA256 combined with the compatibility of SHA1 at a performance hit of only 20\% when compared to only using SHA1.
Given the current compatibility needs and the expectation that fast solid-state storage will continue to shift the balance between IO and hashing performance, we propose that our own opportunistic hashing functionality, but possibly also digital forensics as a field consider the following migration path away from the usage of SHA1.
\begin{itemize}
\item \emph{legacy mode} : SHA1 only (100\% speed; compatible/insecure)
\item \emph{transitional mode} Dual hashing (80\% speed; compatible/secure)
\item \emph{performance mode} BLAKE2bp only (420\% speed; incompatible/secure)
\end{itemize}
\section{Conclusion}
As traditionally used secure hashing algorithms seem to accelerate in the need for their deprecation, and while technically not as practically broken for our purposes as to make it urgent, the reality of the impact that lack of understanding of the impact of the meaning of this deprecation may have in a court of law changes the urgency for finding a migration path away from MD5 and SHA1. The multi-core variant of the BLAKE2 algorithm appears to allow for a migration path that at first includes the sustained use of SHA1 during a transitional period while allowing for hash-set providers to catch up. The migration path also allows the digital forensic process to become prepared for a time when the balance between IO and hashing performance shifts more towards hashing being the primary bottleneck.  We shall be using this migration path in our implementation of the MattockFS opportunistic hashing functionality. The findings in this appendix also justify us advocating the application of this migration path to the whole field of digital forensics. 
